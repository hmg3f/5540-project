{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99195332-512e-4632-988b-e4862f3fff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in ./venv/lib/python3.13/site-packages (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: contractions in ./venv/lib/python3.13/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in ./venv/lib/python3.13/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in ./venv/lib/python3.13/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in ./venv/lib/python3.13/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: num2words in ./venv/lib/python3.13/site-packages (0.5.14)\n",
      "Requirement already satisfied: docopt>=0.6.2 in ./venv/lib/python3.13/site-packages (from num2words) (0.6.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in ./venv/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.13/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-19.0.1-cp313-cp313-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "!pip install contractions\n",
    "!pip install num2words\n",
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef4c9f0a-8b63-49b1-9bf6-93b44b8d1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q http://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.1-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f06cba2-aee7-4387-89b3-a1ccec6c79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/default\"\n",
    "os.environ[\"SPARK_HOME\"] = \"spark-3.5.1-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2db29f3-4bc5-4175-b3ad-56f903485041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "282e35c2-acb3-462e-9803-f455e2995eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/21 22:37:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/21 22:37:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://euclid:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ac8f1193a10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f76473e-097b-4787-bd66-b04ab8d2ce7f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84071954-b4ec-403c-82da-6847a2a37ea3",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f689671-427e-4045-b5c9-08eacfdd5d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Target: integer (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/tweets.csv', header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014ef7c8-a839-44b1-b343-8a2a050970d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------------------+--------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|Target|ID        |Date                        |flag    |User         |Text                                                                                                           |\n",
      "+------+----------+----------------------------+--------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|0     |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY|scotthamilton|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!|\n",
      "|0     |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY|mattycus     |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                      |\n",
      "|0     |1467811184|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY|ElleCTF      |my whole body feels itchy and like its on fire                                                                 |\n",
      "|0     |1467811193|Mon Apr 06 22:19:57 PDT 2009|NO_QUERY|Karoli       |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. |\n",
      "|0     |1467811372|Mon Apr 06 22:20:00 PDT 2009|NO_QUERY|joy_wolf     |@Kwesidei not the whole crew                                                                                   |\n",
      "+------+----------+----------------------------+--------+-------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orig = df\n",
    "\n",
    "df_orig.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a2d6e-3d5a-45ce-96ba-7a9060c7893f",
   "metadata": {},
   "source": [
    "## Dataframe Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b9d9374-a826-4c92-b0a7-f9b641230609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+-------------+--------------------+---+----+\n",
      "|Positive|        ID|               Date|         User|                Text|Day|Hour|\n",
      "+--------+----------+-------------------+-------------+--------------------+---+----+\n",
      "|       0|1467810672|2009-04-07 00:19:49|scotthamilton|is upset that he ...|  1|   0|\n",
      "|       0|1467810917|2009-04-07 00:19:53|     mattycus|@Kenichan I dived...|  1|   0|\n",
      "|       0|1467811184|2009-04-07 00:19:57|      ElleCTF|my whole body fee...|  1|   0|\n",
      "|       0|1467811193|2009-04-07 00:19:57|       Karoli|@nationwideclass ...|  1|   0|\n",
      "|       0|1467811372|2009-04-07 00:20:00|     joy_wolf|@Kwesidei not the...|  1|   0|\n",
      "+--------+----------+-------------------+-------------+--------------------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, weekday, hour, col, when\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "df = df.drop('flag')\n",
    "\n",
    "datetime_string = \"EEE MMM dd HH:mm:ss z yyyy\"\n",
    "df = df.withColumn('Date', to_timestamp(df.Date, datetime_string))\n",
    "df = df.withColumn('Day', weekday(df.Date))\n",
    "df = df.withColumn('Hour', hour(df.Date))\n",
    "\n",
    "df = df.withColumnRenamed('Target', 'Positive')\n",
    "df = df.withColumn('Positive', when(col('Positive') == 4, 1).otherwise(0))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb6901db-67f0-4b05-924a-9b1adaf1317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|crime_related|  count|\n",
      "+-------------+-------+\n",
      "|            1|   2225|\n",
      "|            0|1046350|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "crime_keywords = ['murder', 'murderer', 'theft', 'assault', 'robbery', 'stole', 'steal', 'stealing', 'burglary', 'arrest', 'arson', 'drug', 'drugs', 'police', 'suspect', 'violence']\n",
    "\n",
    "df = df.withColumn(\n",
    "    'crime_related',\n",
    "    when(col('Text').rlike(\"(?i)\\\\b(\" + \"|\".join(crime_keywords) + \")\\\\b\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "df.groupBy('crime_related').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a005ffd7-1183-44c8-8e0b-6dcae4e4635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Text                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Body Of Missing Northern Calif. Girl Found: Police have found the remains of a missing Northern California girl .. http://tr.im/imji|\n",
      "|augh, eff sarth  i stole some of MIL's nail polish, its pretty! =o                                                                   |\n",
      "|so a murder? gotcha.  Cant believe it                                                                                                |\n",
      "|Megan equals Murder.                                                                                                                 |\n",
      "|@grahamcracker  If only you were working in the Melbourne Victoria Police Department or the Melbourne City Cabs.                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_crime = df.filter(col('crime_related')==1)\n",
    "df_crime.select('Text').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fec76f6-763c-4584-b5cf-74427d697034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+-------------+--------------------+---+----+-------------+---+\n",
      "|Positive|        ID|               Date|         User|                Text|Day|Hour|crime_related|Zip|\n",
      "+--------+----------+-------------------+-------------+--------------------+---+----+-------------+---+\n",
      "|       0|1467810672|2009-04-07 00:19:49|scotthamilton|is upset that he ...|  1|   0|            0| 25|\n",
      "|       0|1467810917|2009-04-07 00:19:53|     mattycus|@Kenichan I dived...|  1|   0|            0| 23|\n",
      "|       0|1467811184|2009-04-07 00:19:57|      ElleCTF|my whole body fee...|  1|   0|            0| 31|\n",
      "|       0|1467811193|2009-04-07 00:19:57|       Karoli|@nationwideclass ...|  1|   0|            0| 58|\n",
      "|       0|1467811372|2009-04-07 00:20:00|     joy_wolf|@Kwesidei not the...|  1|   0|            0| 24|\n",
      "+--------+----------+-------------------+-------------+--------------------+---+----+-------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, floor\n",
    "\n",
    "# simulate zip code approximation from IP address\n",
    "df = df.withColumn(\"Zip\", floor(rand() * 70).cast(\"int\"))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c8c89a1-7617-4afb-9019-3d00e161b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:============================================>           (32 + 8) / 40]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+--------------+--------------------+---+----+-------------+---+\n",
      "|Positive|        ID|               Date|          User|                Text|Day|Hour|crime_related|Zip|\n",
      "+--------+----------+-------------------+--------------+--------------------+---+----+-------------+---+\n",
      "|       0|1556829516|2009-04-19 02:33:07|   bumsonseats|@needanewone poli...|  6|   2|            1| 56|\n",
      "|       0|1989513537|2009-06-01 03:49:36|    ryanbeales|Crazy city homele...|  0|   3|            1|  3|\n",
      "|       0|2211485008|2009-06-17 14:54:48|         McZoe|I could murder a ...|  2|  14|            1|  8|\n",
      "|       0|1679336817|2009-05-02 11:06:38|       nabeelq|Landon keeps stea...|  5|  11|            1| 55|\n",
      "|       0|2178537770|2009-06-15 09:50:51|       brajana|OMG Noooooo!!!!  ...|  0|   9|            1| 50|\n",
      "|       0|2215072217|2009-06-17 19:38:07|         xxmah|@viic_x oh, i won...|  2|  19|            0| 22|\n",
      "|       0|1688896931|2009-05-03 14:10:02|        rubyku|@josephdee great ...|  6|  14|            0| 56|\n",
      "|       0|2329092924|2009-06-25 12:20:28|    Daaniellle|Someone call me a...|  3|  12|            0| 19|\n",
      "|       0|2189479019|2009-06-16 02:07:35|Kaira_CFC_1888|Dreading today so...|  1|   2|            0|  9|\n",
      "|       0|1824684537|2009-05-17 04:28:20|    jadedownes|@tommcfly don't s...|  6|   4|            0|  5|\n",
      "|       0|2031341751|2009-06-04 11:40:51|   rachelbaker|@kdc I was lookin...|  3|  11|            0| 26|\n",
      "|       0|2014901649|2009-06-03 05:03:41|      Shanelad|just had a phone ...|  2|   5|            1| 60|\n",
      "|       0|1883203209|2009-05-22 10:10:28|     mikafable|one more day! but...|  4|  10|            1| 68|\n",
      "|       0|2069508198|2009-06-07 18:00:25|         Kesti|Some people can s...|  6|  18|            1| 48|\n",
      "|       0|2184798266|2009-06-15 18:26:13|       jheniya|I'm bored  Too la...|  0|  18|            0|  1|\n",
      "|       0|1824579977|2009-05-17 04:00:19|        kbuech|Marnie Stern � Tr...|  6|   4|            0| 26|\n",
      "|       0|2239563928|2009-06-19 10:41:49|        wensze|Had a fab, relaxi...|  4|  10|            0| 26|\n",
      "|       0|2045609480|2009-06-05 12:58:42| knitta_please|Just yelled &quot...|  4|  12|            0| 36|\n",
      "|       0|2231993061|2009-06-18 21:18:19|        TaLyMe|@buckhollywood Fi...|  3|  21|            0| 69|\n",
      "|       0|2252772590|2009-06-20 08:58:13|james__buckley|In a car with joe...|  5|   8|            1| 67|\n",
      "+--------+----------+-------------------+--------------+--------------------+---+----+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "count_0 = df.filter(df.crime_related == 0).count()\n",
    "count_1 = df.filter(df.crime_related == 1).count()\n",
    "\n",
    "class_ratio = count_1 / count_0\n",
    "\n",
    "df_0 = df.filter(df.crime_related == 0).sample(withReplacement=False, fraction=class_ratio, seed=42)\n",
    "df_1 = df.filter(df.crime_related == 1)\n",
    "\n",
    "df_balanced = df_0.union(df_1)\n",
    "\n",
    "# shuffle rows\n",
    "df_balanced = df_balanced.orderBy(rand())\n",
    "\n",
    "df_balanced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee10313-b256-47f5-bd38-1c0b0200fb41",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6286f70-c64f-44d1-8778-4fdbd45a4ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/antlers/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/antlers/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/antlers/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "import pyarrow\n",
    "import html\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from num2words import num2words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df55b99c-bf35-4a8c-8855-d8c41dafcf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = {\n",
    "    \"mr.\": \"mister\", \"mrs.\": \"misses\", \"dr.\": \"doctor\", \"st.\": \"street\",\n",
    "    \"u.s.\": \"united states\", \"e.g.\": \"for example\", \"i.e.\": \"that is\",\n",
    "    \"vs.\": \"versus\", \"w/\": \"with\", \"w/o\": \"without\", \"n/a\": \"not applicable\",\n",
    "    \"thx\": \"thanks\", \"u\": \"you\", \"wut\": \"what\", \"wtf\": \"what the fuck\",\n",
    "    \"idk\": \"i do not know\", \"luv\": \"love\", \"irl\": \"in real life\"\n",
    "}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d5f9c18-87f3-478a-9885-b26efab49077",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Functions\n",
    "def expand_contractions_and_acronyms(text):\n",
    "    text = contractions.fix(text)\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(acronyms.keys()) + r')\\b', re.IGNORECASE)\n",
    "    text = pattern.sub(lambda m: acronyms.get(m.group(0).lower(), m.group(0)), text)\n",
    "    return text\n",
    "\n",
    "def handle_web_elements(text):\n",
    "    text = re.sub(r'@[\\w_]+', '<user>', text)\n",
    "    text = re.sub(r'#[\\w_]+', '<hashtag>', text)\n",
    "    text = re.sub(r'\\$', ' dollar ', text)\n",
    "    text = re.sub(r'€', ' euro ', text)\n",
    "    text = re.sub(r'http\\S+', 'URL', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def lemmatize_with_pos(tokens):\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for token, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(token, pos=wn_tag) if wn_tag else lemmatizer.lemmatize(token)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def preprocess_text(text, use_spelling_correction=False, convert_numbers=False):\n",
    "    text = text.lower()\n",
    "    text = html.unescape(text)\n",
    "    text = expand_contractions_and_acronyms(text)\n",
    "    text = handle_web_elements(text)\n",
    "    text = re.sub(r'[+-]?(\\d*\\.)?\\d+', lambda m: num2words(m.group()), text)\n",
    "    text = remove_punctuation(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize_with_pos(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c765202-a543-4833-ac65-b56a9b81a21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------------+-----------+--------------------+---+----+-------------+---+--------------------+\n",
      "|Positive|        ID|               Date|       User|                Text|Day|Hour|crime_related|Zip|   Text_Preprocessed|\n",
      "+--------+----------+-------------------+-----------+--------------------+---+----+-------------+---+--------------------+\n",
      "|       0|1556829516|2009-04-19 02:33:07|bumsonseats|@needanewone poli...|  6|   2|            1| 56|user police dog c...|\n",
      "|       0|1989513537|2009-06-01 03:49:36| ryanbeales|Crazy city homele...|  0|   3|            1|  3|crazy city homele...|\n",
      "|       0|2211485008|2009-06-17 14:54:48|      McZoe|I could murder a ...|  2|  14|            1|  8|could murder cold...|\n",
      "|       0|1679336817|2009-05-02 11:06:38|    nabeelq|Landon keeps stea...|  5|  11|            1| 55|landon keep steal...|\n",
      "|       0|2178537770|2009-06-15 09:50:51|    brajana|OMG Noooooo!!!!  ...|  0|   9|            1| 50|omg noooooo stupi...|\n",
      "+--------+----------+-------------------+-----------+--------------------+---+----+-------------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def preprocess_text_udf(text_series):\n",
    "    return text_series.apply(preprocess_text)\n",
    "\n",
    "df_balanced = df_balanced.withColumn(\"Text_Preprocessed\", preprocess_text_udf(df_balanced[\"Text\"]))\n",
    "df_balanced.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b63bf7-e629-4a98-b194-5078e26608e5",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49a8fb-5b8f-4b93-a303-f8666a069054",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e6de7-bcb8-4bc9-9bcb-f2fe683f8a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38c6ef5c-bc11-4a75-a256-c6bb806cd838",
   "metadata": {},
   "source": [
    "## Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5a27a-ef40-4bb0-87ab-f7ba79283a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipy_CRS",
   "language": "python",
   "name": "ipy_crs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
